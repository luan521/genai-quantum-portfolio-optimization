import torch
import torch.nn as nn
from transformers import GPT2Config, GPT2Model
import numpy as np
from ai_quantum.quantum.qaoa import QAOA

class GPT2_QAOA(nn.Module):
    """
    Implements Generative QAOA (GQAOA) for portfolio optimization using transformers.GPT2Model to 
    optimize QAOA circuit parameters.
    
    Attributes:
        vocab_size: (int) Discretization size for the parameters gamma, beta of the QAOA circuit
        max_depth: (int) Maximum depth of QAOA that will be generated
        expected_value (list[float]): List of asset returns.
        cov_matrix (DataFrame-like): Covariance matrix.
        q (float): Covariance scaling factor.
        B (float): Budget/threshold parameter.
        lamb (float): Penalty parameter.
        qc (qiskit.circuit.quantumcircuit.QuantumCircuit): Quantum circuit already initialized.
        mixture_layer (str): x, ring_mixer.
        q_graph (list): list of tuples containing all connected qubits
        n_embd (int): Dimensionality of the embeddings and hidden states.
        n_layer (int): Number of hidden layers in the Transformer encoder.
        n_head (int): Number of attention heads for each attention layer in the Transformer encoder.
    """
    
    def __init__(self, 
                 vocab_size, 
                 max_depth, 
                 expected_value, 
                 cov_matrix, 
                 q, 
                 B, 
                 lamb, 
                 qc=None, 
                 mixture_layer='x', 
                 q_graph=None,
                 n_embd=192, 
                 n_layer=4, 
                 n_head=8):
        """
        Initializes the GQAOA instance.
        
        Args:
            vocab_size: (int) Discretization size for the parameters gamma, beta of the QAOA circuit
            max_depth: (int) Maximum depth of QAOA that will be generated
            expected_value (list[float]): List of asset returns.
            cov_matrix (DataFrame-like): Covariance matrix.
            q (float): Covariance scaling factor.
            B (float): Budget/threshold parameter.
            lamb (float): Penalty parameter.
            qc (qiskit.circuit.quantumcircuit.QuantumCircuit): Quantum circuit already initialized.
            mixture_layer (str): x, ring_mixer.
            q_graph (list): list of tuples containing all connected qubits
            n_embd (int): Dimensionality of the embeddings and hidden states.
            n_layer (int): Number of hidden layers in the Transformer encoder.
            n_head (int): Number of attention heads for each attention layer in the Transformer encoder.
        """
        
        # GPT model
        super(GPT2_QAOA, self).__init__()
        config = GPT2Config(
            vocab_size=vocab_size + 1,   
            n_positions=max_depth * 2,     
            n_ctx=max_depth * 2,
            n_embd=n_embd,
            n_layer=n_layer,
            n_head=n_head,
        )
        self.transformer = GPT2Model(config)
        self.lm_head = nn.Linear(n_embd, vocab_size)
        self._initialize_weights()
        
        # Quantum circuit parameters
        self.expected_value = expected_value
        self.cov_matrix = cov_matrix
        self.q = q
        self.B = B
        self.lamb = lamb
        self.qc = qc
        self.mixture_layer = mixture_layer
        self.q_graph = q_graph

        # Define mappings from tokens to angles for QAOA.
        self.vocab_gamma = np.linspace(0, 2*np.pi, vocab_size)
        self.vocab_beta = np.linspace(0, 2*np.pi, vocab_size)
        
    def _initialize_weights(self):
        """
        Weight initialization for the transformer and lm_head.
        """
        
        for module in self.transformer.modules():
            if isinstance(module, nn.Linear):
                #nn.init.xavier_uniform_(module.weight)
                nn.init.normal_(module.weight, mean=0, std=0.02)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, mean=0, std=0.02)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.02)
        #nn.init.xavier_uniform_(self.lm_head.weight)
        nn.init.zeros_(self.lm_head.bias)
    
    def forward(self, input_ids, beta_temp, full_input_ids=None):
        """
        Predict the next id of the QAOA parameter.
        
        Args:
            input_ids: (torch.Tensor [1, 1]) The ids already generated
            beta_temp: (float) The inverse of temperature, controll the randomness of the stochastic process
            full_input_ids: (List(torch.Tensor [1])) Optional. The already set indexes that will be generated by the model. In that case, 
            the model only produces different logits values, depending in the current state. 
            
        Returns:
            (float, int): Logit values for each predicted id and the id generated by stochastic process
        """
        
        outputs = self.transformer(input_ids)
        hidden_states = outputs.last_hidden_state
        hidden_states = hidden_states[:, -1, :]
        logits = self.lm_head(hidden_states)
        logits = logits.view(1, -1)
        
        pred_probabilities = torch.softmax(-beta_temp*logits, dim=1)[0]
        sample_index = torch.multinomial(pred_probabilities, num_samples=1)
        if full_input_ids is not None:
            sample_index = full_input_ids[input_ids.shape[1]-1]
        
        response = logits[0][sample_index], (sample_index+1).view(1, 1)
        return response
    
    def generate_parameter_sequence(self, beta_temp, depth, full_input_ids=None, start_token=0):
        """
        Autoregressively generate the sequence of ids of length 2 * depth, for the QAOA circuit parameters.
        
        Args:
            beta_temp: (float) The inverse of temperature, controll the randomness of the stochastic process
            depth: (int) QAOA depth
            full_input_ids: (List(torch.Tensor [1])) Optional. The already set indexes that will be generated by the model. In that case, 
            the model only produces different logits values, depending in the current state. 
            start_token: (int) The first token that will start the process
            
        Returns:
            (List(tuple)): Logits and ids generated for each QAOA layer
        
        """
        input_ids = torch.tensor([[start_token]], dtype=torch.long)
        data = [self.forward(input_ids, beta_temp, full_input_ids)]
        for i in range(2*depth-1):
            input_ids = torch.cat((input_ids, data[-1][1]), dim=1)
            data.append(self.forward(input_ids, beta_temp, full_input_ids))
        return data

    def forward_qc(self, beta_temp, depth, full_input_ids=None, precision=1e-3):
        """
        Call self.forward to predict the QAOA circuit parameters and then call the QPU to run the circuit and
        get the measured energy.
        
        Args:
            beta_temp: (float) The inverse of temperature, controll the randomness of the stochastic process
            depth: (int) QAOA depth
            full_input_ids: (List(torch.Tensor [1])) Optional. The already set indexes that will be generated by the model. In that case, 
            the model only produces different logits values, depending in the current state. 
            precision: (float) Precision for the energy measurement.
            
        Returns:
            (float, torch.tensor, List, List, List): Sum of the logits referring to the circuit parameter IDs;
            Measured energy; Ids of the QAOA parameters; Gamma QAOA parameters; Beta QAOA parameters.
        
        """
        
        data = self.generate_parameter_sequence(beta_temp, depth, full_input_ids)
        if full_input_ids is not None:
            qaoa = QAOA(self.expected_value, self.cov_matrix, self.q, self.B, self.lamb, self.qc, self.mixture_layer, self.q_graph)
        sum_w = 0
        gamma_array = []
        beta_array = []
        
        full_input_ids_response = []
        for i in range(2*depth):
            full_input_ids_response.append(data[i][1][0]-1)
        
        for i in range(depth):
            gamma = self.vocab_gamma[full_input_ids_response[2*i]]
            beta = self.vocab_beta[full_input_ids_response[2*i+1]]
            if full_input_ids is not None:
                qaoa.add_layer(gamma, beta)
            sum_w += data[2*i][0]
            sum_w += data[2*i+1][0]
            gamma_array.append(gamma)
            beta_array.append(beta)
        
        if full_input_ids is not None:
            energy = qaoa.measure_energy(precision=precision)
        else:
            energy = np.nan
        
        return sum_w, torch.tensor([energy]), full_input_ids_response, gamma_array, beta_array
    
    def get_counts_qc(self, beta_temp, depth, full_input_ids=None, shots=1000):
        """
        Call self.forward to predict the QAOA circuit parameters and then call the QPU to run the circuit with function QAOA.get_counts
        
        Args:
            beta_temp: (float) The inverse of temperature, controll the randomness of the stochastic process
            depth: (int) QAOA depth
            full_input_ids: (List(torch.Tensor [1])) Optional. The already set indexes that will be generated by the model. In that case, 
            the model only produces different logits values, depending in the current state. 
            shots: (int) Number of the QPU executions.
            
        Returns:
            (dict): Count for each measured state.
        """
        
        data = self.generate_parameter_sequence(beta_temp, depth, full_input_ids)
        qaoa = QAOA(self.expected_value, self.cov_matrix, self.q, self.B, self.lamb, self.qc, self.mixture_layer, self.q_graph)   
        for i in range(depth):
            gamma = self.vocab_gamma[data[2*i][1][0]-1]
            beta = self.vocab_beta[data[2*i+1][1][0]-1]
            qaoa.add_layer(gamma, beta)
        response = qaoa.get_counts(shots=shots)
        
        return response